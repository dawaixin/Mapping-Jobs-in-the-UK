{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import sqlalchemy as alch\n",
    "import numpy as np\n",
    "import re, nltk, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, output_file, show, output_notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "engine = alch.create_engine('sqlite:///jobs_preprocessed.db', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=24,learning_decay=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve 250k entries from the database\n",
    "entries = 250000\n",
    "# initialise a list to the right size for performance reasons\n",
    "descriptions = [None]*entries\n",
    "\n",
    "i=0\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(\"SELECT jobDescription FROM jobs LIMIT 250000\")\n",
    "    for row in result:\n",
    "        descriptions[i] = row[0]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        \n",
    "                             stop_words='english',          \n",
    "                             lowercase=True,                   \n",
    "                             token_pattern='[a-zA-Z0-9]{3,}'\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=24, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lda.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the variable\n",
    "pickle.dump( lda, open( \"best_lda_model_250k_24t.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the variable\n",
    "lda = pickle.load( open( \"best_lda_model_250k_24t.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "engine_new = alch.create_engine('sqlite:///jobs_LDA_24.db', echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = alch.MetaData()\n",
    "jobs = alch.Table('jobs', metadata,\n",
    "              alch.Column('jobid', alch.Integer, primary_key=True),\n",
    "              alch.Column('jobTopic', alch.Integer)\n",
    "             )\n",
    "metadata.create_all(engine_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine_new.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(\"SELECT COUNT(*) FROM jobs\")\n",
    "    for row in result:\n",
    "        count = row[0]\n",
    "    \n",
    "# initialise a list to the right size for performance reasons\n",
    "descriptions = [None]*count\n",
    "\n",
    "i=0\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(\"SELECT jobDescription FROM jobs\")\n",
    "    for row in result:\n",
    "        descriptions[i] = row[0]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#data_vectorized = vectorizer.fit_transform(descriptions[:250000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_vectorized = vectorizer.transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformed = lda.transform(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.20512821e-04 3.52724151e-01 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.16405816e-01 3.24139264e-01\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04]\n"
     ]
    }
   ],
   "source": [
    "print(transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.20512821e-04 3.52724151e-01 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.16405816e-01 3.24139264e-01\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04\n",
      " 3.20512821e-04 3.20512821e-04 3.20512821e-04 3.20512821e-04]\n"
     ]
    }
   ],
   "source": [
    "print(transformed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [None]*count\n",
    "i=0\n",
    "for element in transformed:\n",
    "    index = np.argmax(element)\n",
    "    indexes[i] = index\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 7, 18, 7, 15, 15, 16, 0, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "print(indexes[152:162])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 entries parsed\n",
      "200000 entries parsed\n",
      "300000 entries parsed\n",
      "400000 entries parsed\n",
      "500000 entries parsed\n",
      "600000 entries parsed\n",
      "700000 entries parsed\n",
      "800000 entries parsed\n",
      "900000 entries parsed\n",
      "1000000 entries parsed\n",
      "1100000 entries parsed\n",
      "1200000 entries parsed\n",
      "1300000 entries parsed\n",
      "1400000 entries parsed\n",
      "1500000 entries parsed\n",
      "1600000 entries parsed\n",
      "1700000 entries parsed\n",
      "1800000 entries parsed\n",
      "1900000 entries parsed\n",
      "2000000 entries parsed\n",
      "2100000 entries parsed\n",
      "2200000 entries parsed\n",
      "2300000 entries parsed\n",
      "2400000 entries parsed\n",
      "2500000 entries parsed\n",
      "2600000 entries parsed\n",
      "2700000 entries parsed\n",
      "2800000 entries parsed\n",
      "2900000 entries parsed\n",
      "3000000 entries parsed\n",
      "3100000 entries parsed\n",
      "3200000 entries parsed\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i=0\n",
    "f=0\n",
    "entries = [None]*count\n",
    "with engine.connect() as con:\n",
    "\n",
    "    rs = con.execute('SELECT jobid FROM jobs')\n",
    "\n",
    "    for row in rs:\n",
    "        \n",
    "        try:      \n",
    "            entry = {'jobid': row[0],'jobTopic': int(indexes[i])}\n",
    "            entries[i] = entry\n",
    "        except:\n",
    "            f +=1\n",
    "\n",
    "        i+=1        \n",
    "        if i %100000 == 0:\n",
    "            print(i,\"entries parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x1b480c96e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "conn.execute(jobs.insert(), entries)\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
